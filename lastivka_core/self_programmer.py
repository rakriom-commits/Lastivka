# AUTOGENERATED header kept for clarity in code blocks above
# (весь файл помістив у цей блок)

"""
Lastivka — Self-Programmer (Release V3.9)

Purpose:
    Safe meta-programming pipeline that lets Lastivka GENERATE → TEST → PROMOTE
    small Python modules ("leaf tools") without touching core files.

Key ideas:
    • No in-memory self-modifying code. All changes happen as new modules under tools/.
    • Three states: proposal/ (staging) → sandbox/ (auto-tests) → active/ (promoted).
    • Hard safety rails: stdlib-only, no network, no file writes outside tools/*.
    • Every generated module must expose: META, selftest(), and run(**kwargs).
    • Configurable safety policy via tools/config.json; plugin templates via tools/plugins/.
    • Optional Windows resource limits via Job Objects when pywin32 is available.
    • Guarded aggregator: policy-checked import from tools/active at runtime.
    • Localized logs (uk/en) via config["lang"] with validation and cached config.
    • Cached plugins & config with automatic invalidation on file changes.
    • Resource monitoring (peak RSS & CPU time) during tests if psutil is available.
    • Configurable logging level via config.json: {"log_level": "INFO"}.
    • Stronger AST-compat (3.8–3.12): literal string folding + safe Subscript parsing.
    • Perf telemetry: store light timing stats in tools/cache/*_perf.json (bounded size).

Directory layout (auto-created on first run):
    lastivka_core/tools/
        proposal/   # freshly generated code awaiting tests
        sandbox/    # isolated copy used for execution & tests
        active/     # promoted modules that Lastivka may import & use
        cache/      # cached test results & perf telemetry
        plugins/    # optional codegen templates (safe-only snippets)
        config.json # policy, timeouts, etc.

CLI examples:
    python -m lastivka_core.self_programmer propose --name sum2 --goal "sum and sort"
    python -m lastivka_core.self_programmer test --name sum2 --timeout 10
    python -m lastivka_core.self_programmer promote --name sum2
    python -m lastivka_core.self_programmer list
    python -m lastivka_core.self_programmer remove --name sum2 --stage proposal
    python -m lastivka_core.self_programmer export --stage active --output tools_active.tar.gz
    python -m lastivka_core.self_programmer import --archive tools_active.tar.gz --stage active
    python -m lastivka_core.self_programmer interactive

Integration:
    from lastivka_core.self_programmer import AutoProgrammer
    ap = AutoProgrammer(); ap.promote_if_ok("sum2")

!!! All file writes are constrained to tools/* subfolders. !!!
"""
from __future__ import annotations

import argparse
import ast
import hashlib
import io
import json
import logging
import platform
import re
import shutil
import subprocess
import sys
import tarfile
import textwrap
import time
from dataclasses import dataclass
from pathlib import Path
from threading import Thread
from typing import Dict, Optional

# ============================= logging & paths ================================
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

BASE = Path(__file__).resolve().parent  # .../lastivka_core
ROOT = BASE.parent  # project root containing lastivka_core
TOOLS = BASE / "tools"
PROPOSAL = TOOLS / "proposal"
SANDBOX = TOOLS / "sandbox"
ACTIVE = TOOLS / "active"
CACHE = TOOLS / "cache"
PLUGINS = TOOLS / "plugins"
CONFIG_FILE = TOOLS / "config.json"
AGGREGATOR = ACTIVE / "__init__.py"
TMP_IMPORT = TOOLS / "_tmp_import"  # atomic import staging

# ============================= i18n / messages ===============================

MESSAGES = {
    "en": {
        "init_dirs": "Ensured directory structure, aggregator, and default config",
        "loaded_cfg": "Loaded configuration from tools/config.json",
        "cfg_load_fail": "Failed to load config.json: {err}",
        "cfg_invalid_lang": "Invalid language in config.json: {lang}. Falling back to 'uk'.",
        "cfg_log_level": "Log level set to {lvl}",
        "cfg_schema_warn": "Invalid type for '{key}', expected {etype}; using default",
        "goal_empty": "Goal cannot be empty",
        "proposal_exists": "Module {name} already exists in proposal",
        "policy_violation_gen": "Policy violation while generating {name}: {reason}",
        "proposal_written": "Written proposal module: {path}",
        "module_not_found": "Module {name} not found in proposal",
        "tests_ok_cache": "Tests OK (from cache) for {mod}",
        "tests_fail_cache": "Tests FAIL (from cache) for {mod}",
        "tests_timeout": "Tests timed out for {mod}",
        "tests_passed": "Tests passed for {mod}",
        "tests_failed": "Tests failed for {mod}: {stderr}",
        "test_error": "Test error for {mod}: {err}",
        "tests_resource_used": "Test {mod} peak RSS ≈ {mb:.2f} MB",
        "tests_resource_used_ext": "Test {mod}: peak RSS ≈ {mb:.2f} MB, CPU ≈ {cpu:.2f}s",
        "promote_violation": "Policy violation in {path}: {reason}",
        "promoted": "Promoted {mod} → {dst}",
        "cannot_promote": "Cannot promote {name}: tests failed",
        "promotion_failed": "Promotion failed for {name}: {err}",
        "removed_proposal": "Removed module {name} from proposal",
        "proposal_missing": "Module {name} not found in proposal",
        "removed_active": "Removed all versions of {name} from active and aggregator",
        "no_active_versions": "No active versions found for {name}",
        "exported": "Exported stage '{stage}' to {out}",
        "imported": "Imported modules into stage '{stage}' from {archive}",
        "plugin_cache_refreshed": "Plugin cache refreshed (hash changed)",
        "config_cache_refreshed": "Config cache refreshed (hash changed)",
        "op_time": "{op}('{name}') finished in {seconds:.3f}s",
    },
    "uk": {
        "init_dirs": "Забезпечено структуру тек, агрегатор і типовий конфіг",
        "loaded_cfg": "Конфігурацію завантажено з tools/config.json",
        "cfg_load_fail": "Не вдалося завантажити config.json: {err}",
        "cfg_invalid_lang": "Некоректна мова в config.json: {lang}. Повертаюся до 'uk'.",
        "cfg_log_level": "Рівень логування встановлено на {lvl}",
        "cfg_schema_warn": "Некоректний тип для '{key}', очікується {etype}; використовую типовий",
        "goal_empty": "Мета (goal) не може бути порожньою",
        "proposal_exists": "Модуль {name} вже існує у proposal",
        "policy_violation_gen": "Порушення політики під час генерації {name}: {reason}",
        "proposal_written": "Записано модуль-пропозицію: {path}",
        "module_not_found": "Модуль {name} не знайдено у proposal",
        "tests_ok_cache": "Тести ОК (із кешу) для {mod}",
        "tests_fail_cache": "Тести НЕ пройшли (із кешу) для {mod}",
        "tests_timeout": "Тести перевищили час для {mod}",
        "tests_passed": "Тести пройшли для {mod}",
        "tests_failed": "Тести не пройшли для {mod}: {stderr}",
        "test_error": "Помилка під час тестів {mod}: {err}",
        "tests_resource_used": "Тест {mod} піковий RSS ≈ {mb:.2f} МБ",
        "tests_resource_used_ext": "Тест {mod}: піковий RSS ≈ {mb:.2f} МБ, CPU ≈ {cpu:.2f}с",
        "promote_violation": "Порушення політики в {path}: {reason}",
        "promoted": "Промоут {mod} → {dst}",
        "cannot_promote": "Не можу промоутити {name}: тести не пройшли",
        "promotion_failed": "Промоут не вдався для {name}: {err}",
        "removed_proposal": "Видалено модуль {name} з proposal",
        "proposal_missing": "Модуль {name} не знайдено у proposal",
        "removed_active": "Видалено всі версії {name} з active та агрегатора",
        "no_active_versions": "Активних версій для {name} не знайдено",
        "exported": "Експортовано стадію '{stage}' до {out}",
        "imported": "Імпортовано модулі у стадію '{stage}' з {archive}",
        "plugin_cache_refreshed": "Кеш плагінів оновлено (змінився хеш)",
        "config_cache_refreshed": "Кеш конфіга оновлено (змінився хеш)",
        "op_time": "{op}('{name}') завершено за {seconds:.3f}с",
    },
}

# Config cache (hash-based)
_CONFIG_CACHE: Optional[Dict] = None
_CONFIG_HASH: Optional[str] = None

def _msg(key: str, **kwargs) -> str:
    lang = (_CONFIG_CACHE or {}).get("lang", "uk")
    tpl = MESSAGES.get(lang, MESSAGES["en"]).get(key, key)
    return tpl.format(**kwargs)

def log_info(key: str, **kwargs) -> None:
    logger.info(_msg(key, **kwargs))

def log_warning(key: str, **kwargs) -> None:
    logger.warning(_msg(key, **kwargs))

def log_error(key: str, **kwargs) -> None:
    logger.error(_msg(key, **kwargs))

def log_debug(key: str, **kwargs) -> None:
    logger.debug(_msg(key, **kwargs))

# ============================= policy constants ==============================

FORBIDDEN_BUILTINS = {
    "eval", "exec", "open", "__import__", "input",
    "compile", "memoryview", "globals", "locals"
}

def _default_config() -> Dict:
    return {
        "safe_imports": [
            "math", "statistics", "functools", "itertools", "operator",
            "re", "json", "typing", "collections", "dataclasses", "datetime",
            "fractions", "decimal", "random", "hashlib"
        ],
        "forbidden_patterns": [
            r"\b(import\s+os)\b",
            r"\b(import\s+subprocess)\b",
            r"\b(import\s+socket)\b",
            r"\b(import\s+ctypes)\b",
            r"\b(import\s+threading)\b",
            r"\b(import\s+gc)\b",
            r"\b(import\s+inspect)\b",
            r"\b(open\s*\()",
            r"\b(exec\s*\()",
            r"\b(eval\s*\()",
            r"\b(__import__\s*\()",
            r"\b(sys\.|os\.)\w+",
        ],
        "extra_forbidden_patterns": [],
        "default_timeout": 10,
        "max_file_size_bytes": 131072,  # 128 KiB
        "max_lines": 1500,
        "lang": "uk",
        "log_level": "INFO",
        "monitor_interval_sec": 0.1,
        "perf_history_limit": 100,
    }

def _file_hash(path: Path) -> str:
    if not path.exists():
        return ""
    return hashlib.sha256(path.read_bytes()).hexdigest()

def _validate_config(cfg: Dict) -> Dict:
    """Light schema/type check; coerce to defaults when invalid."""
    schema = {
        "safe_imports": list,
        "forbidden_patterns": list,
        "extra_forbidden_patterns": list,
        "default_timeout": int,
        "max_file_size_bytes": int,
        "max_lines": int,
        "lang": str,
        "log_level": str,
        "monitor_interval_sec": (int, float),
        "perf_history_limit": int,
    }
    defaults = _default_config()
    out = dict(defaults)
    for k, t in schema.items():
        if k in cfg:
            v = cfg[k]
            ok = isinstance(v, t) if isinstance(t, tuple) else isinstance(v, t)
            if ok and isinstance(v, list):
                ok = all(isinstance(x, str) for x in v)
            if ok:
                out[k] = v
            else:
                etype = "/".join(tt.__name__ for tt in t) if isinstance(t, tuple) else t.__name__
                log_warning("cfg_schema_warn", key=k, etype=etype)
    for k, v in cfg.items():
        if k in out:
            continue
        if isinstance(v, (str, int, float, bool, list, dict)) or v is None:
            out[k] = v
    return out

def load_config() -> Dict:
    """Load and cache config.json; invalidate cache; validate schema; apply log level."""
    global _CONFIG_CACHE, _CONFIG_HASH
    current_hash = _file_hash(CONFIG_FILE)
    if _CONFIG_CACHE is not None and _CONFIG_HASH == current_hash:
        return _CONFIG_CACHE

    default = _default_config()
    merged = default
    if CONFIG_FILE.exists():
        try:
            with CONFIG_FILE.open("r", encoding="utf-8") as f:
                user = json.load(f)
            merged = _validate_config(user)
            if merged.get("lang") not in MESSAGES:
                log_warning("cfg_invalid_lang", lang=merged.get("lang"))
                merged["lang"] = default["lang"]
            log_info("loaded_cfg")
        except json.JSONDecodeError as e:
            log_warning("cfg_load_fail", err=f"JSON decode error: {e}")
            merged = default
        except OSError as e:
            log_warning("cfg_load_fail", err=e)
            merged = default

    lvl_name = merged.get("log_level", "INFO")
    lvl = getattr(logging, str(lvl_name).upper(), logging.INFO)
    logging.getLogger().setLevel(lvl)
    log_info("cfg_log_level", lvl=str(lvl_name).upper())

    _CONFIG_CACHE = merged
    _CONFIG_HASH = current_hash
    return merged

CONFIG = load_config()
SAFE_IMPORTS = set(CONFIG["safe_imports"])  # type: ignore
FORBIDDEN_PATTERNS = list(CONFIG["forbidden_patterns"])  # type: ignore
EXTRA_FORBIDDEN_PATTERNS = list(CONFIG.get("extra_forbidden_patterns", []))  # type: ignore
DEFAULT_TIMEOUT = int(CONFIG.get("default_timeout", 10))
MAX_FILE_SIZE = int(CONFIG.get("max_file_size_bytes", 131072))
MAX_LINES = int(CONFIG.get("max_lines", 1500))
MONITOR_INTERVAL = float(CONFIG.get("monitor_interval_sec", 0.1))
PERF_LIMIT = int(CONFIG.get("perf_history_limit", 100))
FORBIDDEN_PATTERNS = FORBIDDEN_PATTERNS + EXTRA_FORBIDDEN_PATTERNS  # merge

# =============================== utilities ===================================

def ensure_dirs() -> None:
    for p in (TOOLS, PROPOSAL, SANDBOX, ACTIVE, CACHE, PLUGINS):
        p.mkdir(parents=True, exist_ok=True)
    for pkg in (TOOLS, PROPOSAL, SANDBOX, ACTIVE):
        init = pkg / "__init__.py"
        if not init.exists():
            init.write_text("# pkg\n", encoding="utf-8")
    if not AGGREGATOR.exists():
        _write_aggregator_lines(["# initial"])
    if not CONFIG_FILE.exists():
        CONFIG_FILE.write_text(json.dumps(_default_config(), indent=2), encoding="utf-8")
    log_info("init_dirs")

def slugify(name: str) -> str:
    s = re.sub(r"[^a-zA-Z0-9_]+", "_", name.strip())
    s = re.sub(r"_+", "_", s)
    return s.strip("_").lower() or "mod"

def _check_size_limits(src_text: str, origin: str) -> None:
    size = len(src_text.encode("utf-8"))
    lines = src_text.count("\n") + 1
    if size > MAX_FILE_SIZE:
        raise ValueError(_msg("size_exceeded", origin=origin, size=size, limit=MAX_FILE_SIZE))
    if lines > MAX_LINES:
        raise ValueError(_msg("lines_exceeded", origin=origin, lines=lines, limit=MAX_LINES))

def _record_perf(kind: str, name: str, seconds: float) -> None:
    """Append timing telemetry to tools/cache/<name>_perf.json (bounded)."""
    try:
        CACHE.mkdir(parents=True, exist_ok=True)
        path = CACHE / f"{slugify(name)}_perf.json"
        data = {"propose": [], "test": [], "promote": []}
        if path.exists():
            try:
                data.update(json.loads(path.read_text(encoding="utf-8")))
            except Exception:
                pass
        arr = data.get(kind, [])
        arr.append({"ts": time.time(), "time": seconds})
        if len(arr) > PERF_LIMIT:
            arr = arr[-PERF_LIMIT:]
        data[kind] = arr
        path.write_text(json.dumps(data, indent=2), encoding="utf-8")
    except Exception:
        pass

@dataclass
class ModuleMeta:
    name: str
    goal: str
    version: int = 1

    @property
    def module_filename(self) -> Path:
        return PROPOSAL / f"{slugify(self.name)}.py"

    @property
    def module_name(self) -> str:
        return slugify(self.name)

# ============================= plugin system =================================

_PLUGINS_CACHE: Optional[Dict[str, Dict[str, str]]] = None
_PLUGINS_HASH: Optional[str] = None

def _plugins_dir_hash() -> str:
    h = hashlib.sha256()
    for p in sorted(PLUGINS.glob("*.py")):
        try:
            h.update(p.read_bytes())
        except OSError:
            pass
    return h.hexdigest()

def _const_only_obj(node: ast.AST):
    """Build Python obj from AST allowing ONLY constant literals & containers."""
    if isinstance(node, ast.Constant):
        return node.value
    if isinstance(node, ast.List):
        return [_const_only_obj(e) for e in node.elts]
    if isinstance(node, ast.Tuple):
        return tuple(_const_only_obj(e) for e in node.elts)
    if isinstance(node, ast.Dict):
        out = {}
        for k, v in zip(node.keys, node.values):
            key = _const_only_obj(k)
            if not isinstance(key, (str, int, float, bool, type(None))):
                raise ValueError("non-constant dict key")
            out[key] = _const_only_obj(v)
        return out
    raise ValueError("non-constant expression")

def _extract_plugin_dict_from_ast(src: str, plugin_file: Path) -> Optional[Dict[str, str]]:
    try:
        tree = ast.parse(src)
    except SyntaxError as e:
        log_warning("plugins_read_fail", fname=plugin_file.name, err=f"{type(e).__name__}: {e}")
        return None
    for node in ast.walk(tree):
        if isinstance(node, ast.Assign) and node.targets and isinstance(node.targets[0], ast.Name) and node.targets[0].id == "PLUGIN":
            try:
                if isinstance(node.value, ast.Dict):
                    plugin_dict = _const_only_obj(node.value)
                    if isinstance(plugin_dict, dict):
                        return plugin_dict  # {"goal": "...", "run_body": "...", "selftest_body": "..."}
            except Exception:
                log_warning("plugins_literal_fail", fname=plugin_file.name)
                return None
    return None

def load_plugins() -> Dict[str, Dict[str, str]]:
    """Load safe plugin snippets via static analysis, with hash-based cache invalidation."""
    global _PLUGINS_CACHE, _PLUGINS_HASH
    current_hash = _plugins_dir_hash()
    if _PLUGINS_CACHE is not None and _PLUGINS_HASH == current_hash:
        return _PLUGINS_CACHE
    if _PLUGINS_CACHE is not None and _PLUGINS_HASH != current_hash:
        log_info("plugin_cache_refreshed")

    plugins: Dict[str, Dict[str, str]] = {}
    for plugin_path in PLUGINS.glob("*.py"):
        try:
            src = plugin_path.read_text(encoding="utf-8")
        except OSError as e:
            log_warning("plugins_read_fail", fname=plugin_path.name, err=f"{type(e).__name__}: {e}")
            continue

        bad = violates_policy(src)
        if bad:
            log_warning("plugins_rejected", fname=plugin_path.name, reason=bad)
            continue

        plug = _extract_plugin_dict_from_ast(src, plugin_path)
        if not isinstance(plug, dict):
            log_warning("plugins_no_dict", fname=plugin_path.name)
            continue

        rb = str(plug.get("run_body", ""))
        sb = str(plug.get("selftest_body", ""))

        if rb and violates_policy(rb):
            log_warning("plugins_rejected", fname=plugin_path.name, reason="run_body policy")
            continue
        if sb and violates_policy(sb):
            log_warning("plugins_rejected", fname=plugin_path.name, reason="selftest_body policy")
            continue

        goal_key = str(plug.get("goal", plugin_path.stem)).lower()
        if goal_key:
            plugins[goal_key] = {"goal": goal_key, "run_body": rb, "selftest_body": sb}
            log_info("plugins_loaded", fname=plugin_path.name)

    _PLUGINS_CACHE = plugins
    _PLUGINS_HASH = current_hash
    return plugins

# ============================= policy enforcement ============================

def _literal_str(node: ast.AST) -> Optional[str]:
    """Return concatenated string if node is statically-resolvable string."""
    if isinstance(node, ast.Constant) and isinstance(node.value, str):
        return node.value
    if isinstance(node, ast.JoinedStr):
        parts = []
        for v in node.values:
            if isinstance(v, ast.Constant) and isinstance(v.value, str):
                parts.append(v.value)
            else:
                return None
        return "".join(parts)
    if isinstance(node, ast.BinOp) and isinstance(node.op, ast.Add):
        left = _literal_str(node.left)
        right = _literal_str(node.right)
        if left is not None and right is not None:
            return left + right
        return None
    Index = getattr(ast, "Index", None)
    if Index is not None and isinstance(node, Index):
        return _literal_str(node.value)
    return None

def _subscript_key_str(key_node: ast.AST) -> Optional[str]:
    """Extract a static string key from Subscript.slice across Python versions."""
    return _literal_str(key_node)

def violates_policy(src: str) -> Optional[str]:
    """Return a human-readable violation string, or None if OK."""
    if not isinstance(src, str):
        return "non-str source"
    for pat in FORBIDDEN_PATTERNS:
        if re.search(pat, src):
            return f"regex violation: {pat}"
    try:
        tree = ast.parse(src)
    except SyntaxError:
        return "invalid syntax"

    allowed = set(SAFE_IMPORTS)

    for node in ast.walk(tree):
        if isinstance(node, ast.Name) and node.id == "__builtins__":
            return "forbidden name: __builtins__"

        if isinstance(node, ast.Call):
            if isinstance(node.func, ast.Name) and node.func.id in FORBIDDEN_BUILTINS:
                return f"forbidden call: {node.func.id}"
            if isinstance(node.func, ast.Attribute) and isinstance(node.func.value, ast.Name) and node.func.value.id == "builtins":
                if node.func.attr in FORBIDDEN_BUILTINS:
                    return f"forbidden call: builtins.{node.func.attr}"
            if (
                isinstance(node, ast.Subscript)
                and isinstance(node.func.value, ast.Name)
                and node.func.value.id == "__builtins__"
            ):
                key_val = _subscript_key_str(node.slice)
                if key_val in FORBIDDEN_BUILTINS:
                    return f"forbidden call: __builtins__['{key_val}']"

        if isinstance(node, ast.Import):
            for alias in node.names:
                root = alias.name.split(".")[0]
                if root not in allowed:
                    return f"forbidden import: {alias.name}"
        if isinstance(node, ast.ImportFrom):
            if node.module is None:
                return "relative import not allowed"
            root = node.module.split(".")[0]
            if root not in allowed:
                return f"forbidden importfrom: {node.module}"
    return None

# =============================== generation ==================================

def _goal_flags(goal: str) -> dict:
    g = goal.lower()
    return {
        "sum": "sum" in g,
        "sort": "sort" in g,
        "multiply": ("multiply" in g) or ("product" in g),
        "text": "text" in g,
    }

def render_module(meta: ModuleMeta) -> str:
    """Generate a safe module tailored to meta.goal or a plugin override."""
    plugins = load_plugins()
    flags = _goal_flags(meta.goal)
    safe_imports_block = "\n".join([f"import {m}" for m in sorted(SAFE_IMPORTS)])

    run_body = "result = {}\n"
    selftest_lines = []

    for pgoal, plug in plugins.items():
        if pgoal in meta.goal.lower():
            run_body = plug.get("run_body", "result = {'echo': META['goal']}")
            selftest_lines.append(plug.get("selftest_body", "assert isinstance(run(), dict)"))
            break
    else:
        if flags["sum"]:
            run_body += """
    if "a" in args and "b" in args and args.get("op") in (None, "sum"):
        a, b = args["a"], args["b"]
        if not all(isinstance(x, (int, float)) for x in (a, b)):
            raise TypeError("a,b must be numbers")
        result["sum"] = a + b
"""
            selftest_lines.append("assert run(a=1, b=2)['sum'] == 3")
        if flags["multiply"]:
            run_body += """
    if "a" in args and "b" in args and args.get("op") in ("mul", "multiply", "product"):
        a, b = args["a"], args["b"]
        if not all(isinstance(x, (int, float)) for x in (a, b)):
            raise TypeError("a,b must be numbers")
        result["product"] = a * b
"""
            selftest_lines.append("assert run(a=2, b=3, op='mul')['product'] == 6")
        if flags["sort"]:
            run_body += """
    if "list" in args:
        lst = args["list"]
        if not isinstance(lst, list):
            raise TypeError("list must be a list")
        result["sorted"] = sorted(lst)
"""
            selftest_lines.append("assert run(list=[3,1,2])['sorted'] == [1,2,3]")
        if flags["text"]:
            run_body += """
    if "text" in args:
        t = str(args["text"]).strip()
        tokens = [tok for tok in re.findall(r"\\w+", t)]
        result.update({"length": len(t), "tokens": len(tokens)})
"""
            selftest_lines.append("assert run(text='hello world')['length'] == 11")
        if not any(flags.values()):
            run_body += "result = {'meta': META}\n"
            selftest_lines.append("assert isinstance(run(), dict)")

    run_body += "\n    return result"
    selftest_body = "\n    ".join(selftest_lines)

    template = f'''# AUTOGENERATED by self_programmer.py — DO NOT EDIT BY HAND
"""
Leaf tool: {meta.module_name}
Goal: {meta.goal}
Policy: stdlib-only; forbidden: os, subprocess, socket, file I/O, eval/exec/compile/globals/locals/memoryview.
"""
{safe_imports_block}

META = {{
    "name": "{meta.module_name}",
    "goal": {meta.goal!r},
    "version": {meta.version},
    "policy": "safe-leaf",
}}


def _validate_inputs(args: dict) -> dict:
    if not isinstance(args, dict):
        raise TypeError("run() expects kwargs dict")
    return args


def selftest():
    assert isinstance(META["name"], str) and META["name"], "empty name"
    assert META["policy"] == "safe-leaf"
    {textwrap.dedent(selftest_body)}


def run(**kwargs):
    args = _validate_inputs(kwargs)
    {textwrap.dedent(run_body)}
'''
    _check_size_limits(template, f"generated {meta.module_name}")
    bad = violates_policy(template)
    if bad:
        raise ValueError(_msg("policy_violation_gen", name=meta.module_name, reason=bad))
    return template

def write_proposal(meta: ModuleMeta) -> Path:
    if not meta.goal.strip():
        log_error("goal_empty")
        raise ValueError(_msg("goal_empty"))
    if meta.module_filename.exists():
        log_error("proposal_exists", name=meta.module_name)
        raise FileExistsError(_msg("proposal_exists", name=meta.module_name))
    src = render_module(meta)
    _check_size_limits(src, f"proposal {meta.module_name}")
    bad = violates_policy(src)
    if bad:
        log_error("policy_violation_gen", name=meta.module_name, reason=bad)
        raise ValueError(f"policy violation: {bad}")
    meta.module_filename.write_text(src, encoding="utf-8")
    log_info("proposal_written", path=str(meta.module_filename))
    return meta.module_filename

# =============================== testing =====================================

def _clean_sandbox() -> None:
    for f in SANDBOX.glob("**/*.pyc"):
        try: f.unlink()
        except Exception: pass
    for d in SANDBOX.glob("**/__pycache__"):
        shutil.rmtree(d, ignore_errors=True)
    for f in SANDBOX.glob("*.py"):
        try: f.unlink()
        except Exception: pass

def _copy_to_sandbox(mod_path: Path) -> Path:
    SANDBOX.mkdir(parents=True, exist_ok=True)
    _clean_sandbox()
    dst = SANDBOX / mod_path.name
    shutil.copyfile(mod_path, dst)
    return dst

def _attach_windows_job_limits(proc: subprocess.Popen, mem_bytes: int) -> None:
    """Attach Windows Job Object with soft limits if pywin32 is available."""
    if platform.system() != "Windows":
        return
    try:
        import win32job, win32con, win32api  # type: ignore
        hJob = win32job.CreateJobObject(None, "")
        info = win32job.QueryInformationJobObject(hJob, win32job.JobObjectExtendedLimitInformation)
        flags = info["BasicLimitInformation"]["LimitFlags"]
        flags |= (win32job.JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE |
                  win32job.JOB_OBJECT_LIMIT_PROCESS_MEMORY)
        info["BasicLimitInformation"]["LimitFlags"] = flags
        info["ProcessMemoryLimit"] = mem_bytes
        win32job.SetInformationJobObject(hJob, win32job.JobObjectExtendedLimitInformation, info)
        hProc = win32api.OpenProcess(win32con.PROCESS_ALL_ACCESS, False, proc.pid)
        win32job.AssignProcessToJobObject(hJob, hProc)
        log_info("win_job_attached")
    except Exception as e:
        log_warning("plugins_job_warn", err=e)

def _monitor_proc_usage(proc: subprocess.Popen, result: dict) -> None:
    """Background sampler for peak RSS (MB) and CPU time (s), if psutil is present."""
    try:
        import psutil  # type: ignore
        import time as _t
        p = psutil.Process(proc.pid)
        peak_rss = 0.0
        peak_cpu = 0.0
        while proc.poll() is None:
            try:
                mem = p.memory_info().rss / (1024**2)
                ct = p.cpu_times()
                cpu = float(getattr(ct, "user", 0.0)) + float(getattr(ct, "system", 0.0))
                if mem > peak_rss: peak_rss = mem
                if cpu > peak_cpu: peak_cpu = cpu
            except Exception:
                pass
            _t.sleep(max(0.001, float(MONITOR_INTERVAL)))
        result["rss"] = max(result.get("rss", 0.0), peak_rss)
        result["cpu"] = max(result.get("cpu", 0.0), peak_cpu)
    except Exception:
        pass

def run_unittests(mod_basename: str, timeout: int = DEFAULT_TIMEOUT) -> tuple[bool, str]:
    """Run ephemeral tests in isolated subprocess with timeout and light rlimits."""
    mod_path = SANDBOX / f"{mod_basename}.py"
    cache_path = CACHE / f"{mod_basename}.json"
    CACHE.mkdir(parents=True, exist_ok=True)

    src = mod_path.read_text(encoding="utf-8")
    src_hash = hashlib.sha256(src.encode("utf-8")).hexdigest()

    if cache_path.exists():
        try:
            cache = json.loads(cache_path.read_text(encoding="utf-8"))
            if cache.get("hash") == src_hash:
                if cache.get("success"):
                    log_info("tests_ok_cache", mod=mod_basename)
                    return True, "tests: OK (from cache)\n"
                else:
                    log_info("tests_fail_cache", mod=mod_basename)
                    return False, f"tests: FAIL — {cache.get('log', 'Cached failure')}\n"
        except Exception:
            pass

    cpu_seconds = max(1, int(timeout))
    mem_bytes = 256 * 1024 * 1024  # 256 MB

    # >>> PATCHED: make project root visible under -I <<<
    test_code = f'''import importlib, sys, os
# Ensure Lastivka project root is visible in sys.path (Python -I safe)
root = r"{ROOT}"
if root and root not in sys.path:
    sys.path.insert(0, root)

# Light resource limits (POSIX only)
try:
    import resource
    resource.setrlimit(resource.RLIMIT_CPU, ({cpu_seconds}, {cpu_seconds}))
    if hasattr(resource, 'RLIMIT_AS'):
        resource.setrlimit(resource.RLIMIT_AS, ({mem_bytes}, {mem_bytes}))
except Exception:
    pass
m = importlib.import_module("lastivka_core.tools.sandbox.{mod_basename}")
assert hasattr(m, "META") and isinstance(m.META, dict)
assert callable(getattr(m, "selftest"))
assert callable(getattr(m, "run"))
m.selftest()
assert isinstance(m.run(), dict)
print("OK")
'''
    try:
        import resource  # noqa: F401
        _has_posix_limits = True
    except Exception:
        _has_posix_limits = False
    _is_windows = (platform.system() == "Windows")
    if not _is_windows and not _has_posix_limits:
        log_warning("no_posix_limits")

    tfile = SANDBOX / f"test_{mod_basename}.py"
    tfile.write_text(test_code, encoding="utf-8")

    for pkg in (BASE, TOOLS, SANDBOX, ACTIVE):
        ini = pkg / "__init__.py"
        if not ini.exists():
            ini.write_text("# pkg\n", encoding="utf-8")

    try:
        proc = subprocess.Popen(
            [sys.executable, "-I", str(tfile)],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            cwd=str(ROOT),
        )
        _attach_windows_job_limits(proc, mem_bytes)

        # background resource monitor (optional)
        res = {"rss": 0.0, "cpu": 0.0}
        mon = Thread(target=_monitor_proc_usage, args=(proc, res), daemon=True)
        mon.start()

        try:
            stdout, stderr = proc.communicate(timeout=timeout)
        except subprocess.TimeoutExpired:
            proc.kill()
            stdout, stderr = proc.communicate()
            log_error("tests_timeout", mod=mod_basename)
            cache_path.write_text(json.dumps({"hash": src_hash, "success": False, "log": "Timeout exceeded"}), encoding="utf-8")
            return False, "tests: FAIL — Timeout exceeded\n"
        finally:
            waited = 0.0
            while mon.is_alive() and waited < 1.0:
                mon.join(0.05)
                waited += 0.05

        if res.get("rss", 0.0) > 0 or res.get("cpu", 0.0) > 0:
            log_debug("tests_resource_used_ext", mod=mod_basename, mb=res.get("rss", 0.0), cpu=res.get("cpu", 0.0))

        if proc.returncode == 0:
            log_info("tests_passed", mod=mod_basename)
            cache_path.write_text(json.dumps({"hash": src_hash, "success": True}), encoding="utf-8")
            return True, "tests: OK\n" + (stdout or "")
        else:
            log_error("tests_failed", mod=mod_basename, stderr=stderr)
            cache_path.write_text(json.dumps({"hash": src_hash, "success": False, "log": stderr}), encoding="utf-8")
            return False, f"tests: FAIL — {stderr}\n"

    except Exception as e:
        log_error("test_error", mod=mod_basename, err=e)
        cache_path.write_text(json.dumps({"hash": src_hash, "success": False, "log": f"{type(e).__name__}: {e}"}), encoding="utf-8")
        return False, f"tests: FAIL — {type(e).__name__}: {e}\n"

# =============================== promotion ===================================

def _read_aggregator_lines() -> list[str]:
    if not AGGREGATOR.exists():
        return []
    return AGGREGATOR.read_text(encoding="utf-8").splitlines()

def _write_aggregator_lines(lines: list[str]) -> None:
    header = [
        "# auto-generated; do not edit by hand",
        "import importlib, re, ast, pathlib",
        "",
        "# Minimal policy check prior to importing active modules",
        "FORBIDDEN_BUILTINS = {"
        " 'eval','exec','open','__import__','input','compile','memoryview','globals','locals'}",
        "FORBIDDEN_PATTERNS = [",
        "    r\"\\b(import\\s+os)\\b\",",
        "    r\"\\b(import\\s+subprocess)\\b\",",
        "    r\"\\b(import\\s+socket)\\b\",",
        "    r\"\\b(import\\s+ctypes)\\b\",",
        "    r\"\\b(import\\s+threading)\\b\",",
        "    r\"\\b(import\\s+gc)\\b\",",
        "    r\"\\b(import\\s+inspect)\\b\",",
        "    r\"\\b(open\\s*\\()\",",
        "    r\"\\b(exec\\s*\\()\",",
        "    r\"\\b(eval\\s*\\()\",",
        "    r\"\\b(__import__\\s*\\()\",",
        "    r\"\\b(sys\\.|os\\.)\\w+\",",
        "]",
        "",
        "def _violates_policy_quick(src: str):",
        "    for pat in FORBIDDEN_PATTERNS:",
        "        if re.search(pat, src):",
        "            return f'regex violation: {pat}'",
        "    try:",
        "        tree = ast.parse(src)",
        "    except SyntaxError:",
        "        return 'invalid syntax'",
        "    for node in ast.walk(tree):",
        "        if isinstance(node, ast.Call):",
        "            if isinstance(node.func, ast.Name) and node.func.id in FORBIDDEN_BUILTINS:",
        "                return f'forbidden call: {node.func.id}'",
        "            if (isinstance(node.func, ast.Attribute) and",
        "                isinstance(node.func.value, ast.Name) and node.func.value.id == 'builtins' and",
        "                node.func.attr in FORBIDDEN_BUILTINS):",
        "                return f'forbidden call: builtins.{node.func.attr}'",
        "            if (isinstance(node, ast.Call) and isinstance(node.func, ast.Subscript) and",
        "                isinstance(node.func.value, ast.Name) and node.func.value.id == '__builtins__'):",
        "                key = node.func.slice",
        "                if isinstance(key, ast.Constant) and isinstance(key.value, str):",
        "                    key_val = key.value",
        "                    if key_val in FORBIDDEN_BUILTINS:",
        "                        return f\"forbidden call: __builtins__['{key_val}']\"",
        "    return None",
        "",
        "def _safe_import_active(modname: str):",
        "    here = pathlib.Path(__file__).parent",
        "    p = here / f\"{modname}.py\"",
        "    if not p.exists():",
        "        raise ImportError(f'module {modname} not found')",
        "    src = p.read_text(encoding='utf-8')",
        "    bad = _violates_policy_quick(src)",
        "    if bad:",
        "        raise ImportError(f'module {modname} violates policy: {bad}')",
        "    return importlib.import_module(f'lastivka_core.tools.active.{modname}')",
        "",
        "__all__ = []",
        "",
    ]
    body = [l for l in lines if l.strip() and not l.strip().startswith("# initial")]
    AGGREGATOR.write_text("\n".join(header + body) + "\n", encoding="utf-8")

def promote(mod_path: Path) -> Path:
    ACTIVE.mkdir(parents=True, exist_ok=True)

    src = mod_path.read_text(encoding="utf-8")
    bad = violates_policy(src)
    if bad:
        log_error("promote_violation", path=mod_path.name, reason=bad)
        raise ValueError(f"policy violation: {bad}")

    m = re.search(r'"version":\s*(\d+)', src)
    current_version = int(m.group(1)) if m else 1
    new_version = current_version + 1

    modname = mod_path.stem
    dst = ACTIVE / f"{modname}_v{new_version}.py"
    src_bumped = re.sub(r'"version":\s*\d+', f'"version": {new_version}', src)
    dst.write_text(src_bumped, encoding="utf-8")

    lines = _read_aggregator_lines()
    lines = [l for l in lines if not (l.startswith("run_") and f"_{modname}_v" in l)]
    export_line = f"run_{modname}_v{new_version} = _safe_import_active('{modname}_v{new_version}').run"
    if export_line not in lines:
        lines.append(export_line)
    all_line = f"__all__.append('{modname}_v{new_version}')"
    if all_line not in lines:
        lines.append(all_line)
    _write_aggregator_lines(lines)

    log_info("promoted", mod=modname, dst=str(dst))
    return dst

# =============================== public API ==================================

class AutoProgrammer:
    def __init__(self) -> None:
        ensure_dirs()
        log_info("init_dirs")

    def propose(self, name: str, goal: str) -> Path:
        start = time.perf_counter()
        meta = ModuleMeta(name=name, goal=goal)
        out = write_proposal(meta)
        elapsed = time.perf_counter() - start
        log_debug("op_time", op="propose", name=name, seconds=elapsed)
        _record_perf("propose", name, elapsed)
        return out

    def test(self, name: str, timeout: int = DEFAULT_TIMEOUT) -> tuple[bool, str]:
        start = time.perf_counter()
        mod = PROPOSAL / f"{slugify(name)}.py"
        if not mod.exists():
            log_error("module_not_found", name=name)
            raise FileNotFoundError(mod)
        sand = _copy_to_sandbox(mod)
        res = run_unittests(sand.stem, timeout)
        elapsed = time.perf_counter() - start
        log_debug("op_time", op="test", name=name, seconds=elapsed)
        _record_perf("test", name, elapsed)
        return res

    def promote(self, name: str, timeout: int = DEFAULT_TIMEOUT) -> Path:
        start = time.perf_counter()
        mod = PROPOSAL / f"{slugify(name)}.py"
        if not mod.exists():
            log_error("module_not_found", name=name)
            raise FileNotFoundError(mod)
        sand = _copy_to_sandbox(mod)
        ok, logtxt = run_unittests(sand.stem, timeout)
        if not ok:
            log_error("cannot_promote", name=name)
            raise RuntimeError(f"cannot promote — tests failed:\n{logtxt}")
        dst = promote(mod)
        elapsed = time.perf_counter() - start
        log_debug("op_time", op="promote", name=name, seconds=elapsed)
        _record_perf("promote", name, elapsed)
        return dst

    def promote_if_ok(self, name: str, timeout: int = DEFAULT_TIMEOUT) -> tuple[bool, str | Path]:
        try:
            dst = self.promote(name, timeout)
            return True, dst
        except Exception as e:
            log_error("promotion_failed", name=name, err=e)
            return False, f"{type(e).__name__}: {e}"

    def remove(self, name: str, stage: str) -> bool:
        base = slugify(name)
        if stage == "proposal":
            mod = PROPOSAL / f"{base}.py"
            if mod.exists():
                mod.unlink()
                log_info("removed_proposal", name=name)
                return True
            log_error("proposal_missing", name=name)
            return False
        elif stage == "active":
            removed = False
            for f in ACTIVE.glob(f"{base}_v*.py"):
                f.unlink(); removed = True
            if removed:
                lines = _read_aggregator_lines()
                lines = [l for l in lines if base not in l]
                _write_aggregator_lines(lines)
                log_info("removed_active", name=name)
                return True
            log_error("no_active_versions", name=name)
            return False
        else:
            raise ValueError("stage must be 'proposal' or 'active'")

    def export_stage(self, stage: str, output: str) -> Path:
        folder = PROPOSAL if stage == "proposal" else ACTIVE
        out = Path(output)
        with tarfile.open(out, "w:gz") as tar:
            for f in folder.glob("*.py"):
                tar.add(f, arcname=f"tools/{stage}/{f.name}")
        log_info("exported", stage=stage, out=str(out))
        return out

    def import_stage(self, archive: str, stage: str) -> None:
        """Atomically import modules from archive into a stage with policy & size validation."""
        folder = PROPOSAL if stage == "proposal" else ACTIVE
        if TMP_IMPORT.exists():
            shutil.rmtree(TMP_IMPORT, ignore_errors=True)
        TMP_IMPORT.mkdir(parents=True, exist_ok=True)

        extracted: list[tuple[str, str]] = []

        MAX_FILES = 200
        TOTAL_LIMIT = MAX_FILE_SIZE * 10  # total bytes across all files

        try:
            with tarfile.open(archive, "r:gz") as tar:
                total_unpacked = 0
                count_files = 0
                for member in tar.getmembers():
                    # accept only regular *.py files in the exact stage folder
                    if not (member.name.startswith(f"tools/{stage}/") and member.name.endswith(".py")):
                        continue
                    if not member.isreg():
                        continue
                    if member.size > MAX_FILE_SIZE:
                        raise ValueError(f"policy violation on import: {member.name}: file too large ({member.size} > {MAX_FILE_SIZE})")
                    count_files += 1
                    if count_files > MAX_FILES:
                        raise ValueError(f"policy violation on import: too many files (> {MAX_FILES})")
                    if total_unpacked + member.size > TOTAL_LIMIT:
                        raise ValueError(f"policy violation on import: archive too large in sum (> {TOTAL_LIMIT} bytes)")

                    safe_name = Path(member.name).name
                    fobj = tar.extractfile(member)
                    if not fobj:
                        continue
                    data = fobj.read().decode("utf-8", errors="replace")
                    total_unpacked += len(data.encode("utf-8"))
                    extracted.append((safe_name, data))

            violations = []
            for fname, src in extracted:
                try:
                    _check_size_limits(src, f"import {fname}")
                except Exception as e:
                    violations.append((fname, f"size/lines: {e}"))
                    continue
                bad = violates_policy(src)
                if bad:
                    violations.append((fname, bad))
            if violations:
                msgs = "; ".join([f"{fn}: {why}" for fn, why in violations])
                raise ValueError(f"policy violation on import: {msgs}")

            for fname, src in extracted:
                (TMP_IMPORT / fname).write_text(src, encoding="utf-8")

            folder.mkdir(parents=True, exist_ok=True)
            for fname in (f for f, _ in extracted):
                shutil.move(str(TMP_IMPORT / fname), str(folder / fname))

            if stage == "active":
                lines = _read_aggregator_lines()
                for fname in (f for f, _ in extracted):
                    m = re.match(r"^(?P<base>.+)_v(?P<ver>\d+)\.py$", fname)
                    if not m:
                        continue
                    base = m.group("base")
                    ver = m.group("ver")
                    lines = [l for l in lines if not (l.startswith("run_") and f"_{base}_v" in l)]
                    export_line = f"run_{base}_v{ver} = _safe_import_active('{base}_v{ver}').run"
                    all_line = f"__all__.append('{base}_v{ver}')"
                    if export_line not in lines:
                        lines.append(export_line)
                    if all_line not in lines:
                        lines.append(all_line)
                _write_aggregator_lines(lines)

            log_info("imported", stage=stage, archive=archive)
        finally:
            if TMP_IMPORT.exists():
                shutil.rmtree(TMP_IMPORT, ignore_errors=True)

    def list(self) -> dict:
        return {
            "proposal": sorted(p.stem for p in PROPOSAL.glob("*.py")),
            "active": sorted(p.stem for p in ACTIVE.glob("*.py")),
        }

    def interactive(self):
        print("Lastivka Self-Programmer Interactive Mode (type 'exit' to quit)")
        while True:
            try:
                cmd = input("> ").strip()
            except EOFError:
                break
            if cmd == "exit":
                break
            try:
                args = cmd.split()
                if not args:
                    continue
                if args[0] == "propose":
                    if len(args) < 3:
                        print("Usage: propose <name> <goal...>")
                        continue
                    name, goal = args[1], " ".join(args[2:])
                    out = self.propose(name, goal)
                    print(f"[OK] proposal created: {out}")
                elif args[0] == "test":
                    if len(args) < 2:
                        print("Usage: test <name> [timeout]")
                        continue
                    name = args[1]
                    timeout = int(args[2]) if len(args) > 2 and args[2].isdigit() else DEFAULT_TIMEOUT
                    ok, logtxt = self.test(name, timeout)
                    print(logtxt.strip())
                    print(f"[{'OK' if ok else 'FAIL'}] tests for {name}")
                elif args[0] == "promote":
                    if len(args) < 2:
                        print("Usage: promote <name> [timeout]")
                        continue
                    name = args[1]
                    timeout = int(args[2]) if len(args) > 2 and args[2].isdigit() else DEFAULT_TIMEOUT
                    try:
                        out = self.promote(name, timeout)
                        print(f"[OK] promoted to active: {out}")
                    except Exception as e:
                        print(str(e))
                elif args[0] == "remove":
                    if len(args) < 3:
                        print("Usage: remove <name> <stage: proposal|active>")
                        continue
                    ok = self.remove(args[1], args[2])
                    print(f"[{'OK' if ok else 'FAIL'}] removed {args[1]} from {args[2]}")
                elif args[0] == "export":
                    if len(args) < 3:
                        print("Usage: export <stage: proposal|active> <output.tar.gz>")
                        continue
                    path = self.export_stage(args[1], args[2])
                    print(f"[OK] exported {args[1]} to {path}")
                elif args[0] == "import":
                    if len(args) < 3:
                        print("Usage: import <archive.tar.gz> <stage: proposal|active>")
                        continue
                    self.import_stage(args[1], args[2])
                    print(f"[OK] imported into {args[2]} from {args[1]}")
                elif args[0] == "list":
                    info = self.list()
                    print("proposal:", ", ".join(info["proposal"]) or "—")
                    print("active  :", ", ".join(info["active"]) or "—")
                else:
                    print("Unknown command. Try: propose, test, promote, remove, export, import, list")
            except Exception as e:
                print(f"Error: {e}")

# =============================== CLI =========================================

def main(argv=None):
    parser = argparse.ArgumentParser(prog="self_programmer", add_help=True)
    sub = parser.add_subparsers(dest="cmd", required=True)

    p_prop = sub.add_parser("propose", help="generate proposal module")
    p_prop.add_argument("--name", required=True)
    p_prop.add_argument("--goal", required=True)

    p_test = sub.add_parser("test", help="run sandbox tests for proposal")
    p_test.add_argument("--name", required=True)
    p_test.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT)

    p_prom = sub.add_parser("promote", help="promote proposal to active if tests pass")
    p_prom.add_argument("--name", required=True)
    p_prom.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT)

    p_rem = sub.add_parser("remove", help="remove module from proposal or active")
    p_rem.add_argument("--name", required=True)
    p_rem.add_argument("--stage", choices=["proposal", "active"], required=True)

    p_exp = sub.add_parser("export", help="export all modules of a stage to tar.gz")
    p_exp.add_argument("--stage", choices=["proposal", "active"], required=True)
    p_exp.add_argument("--output", required=True)

    p_imp = sub.add_parser("import", help="import modules from tar.gz into a stage")
    p_imp.add_argument("--archive", required=True)
    p_imp.add_argument("--stage", choices=["proposal", "active"], required=True)

    sub.add_parser("list", help="list proposal/active modules")
    sub.add_parser("interactive", help="start interactive mode")

    args = parser.parse_args(argv)
    ap = AutoProgrammer()

    if args.cmd == "propose":
        path = ap.propose(args.name, args.goal)
        print(f"[OK] proposal created: {path}")
    elif args.cmd == "test":
        ok, logtxt = ap.test(args.name, args.timeout)
        print(logtxt.strip())
        print(f"[{'OK' if ok else 'FAIL'}] tests for {args.name}")
        sys.exit(0 if ok else 1)
    elif args.cmd == "promote":
        try:
            out = ap.promote(args.name, args.timeout)
            print(f"[OK] promoted to active: {out}")
        except Exception as e:
            print(e)
            sys.exit(1)
    elif args.cmd == "remove":
        ok = ap.remove(args.name, args.stage)
        print(f"[{'OK' if ok else 'FAIL'}] removed {args.name} from {args.stage}")
        sys.exit(0 if ok else 1)
    elif args.cmd == "export":
        path = ap.export_stage(args.stage, args.output)
        print(f"[OK] exported {args.stage} to {path}")
    elif args.cmd == "import":
        ap.import_stage(args.archive, args.stage)
        print(f"[OK] imported into {args.stage} from {args.archive}")
    elif args.cmd == "list":
        info = ap.list()
        print("proposal:", ", ".join(info["proposal"]) or "—")
        print("active  :", ", ".join(info["active"]) or "—")
    elif args.cmd == "interactive":
        ap.interactive()

if __name__ == "__main__":
    main()
